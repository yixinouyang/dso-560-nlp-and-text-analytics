{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (\"I ate dinner early\", \"HAM\"),\n",
    "    (\"free money today\", \"SPAM\"),\n",
    "    (\"I had a blast\", \"HAM\"),\n",
    "    (\"sign up free early today\", \"HAM\"),\n",
    "    (\"only free today\", \"SPAM\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = set()\n",
    "\n",
    "# Build corpus\n",
    "for document in documents:\n",
    "    text = document[0]\n",
    "    class_value = document[1]\n",
    "    for word in text.split():\n",
    "        corpus.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Conditional Probabilities\n",
    "We need to generate first $P(x|y)$. For instance, what is the likelihood of finding the word `free` if we know the document is `HAM` is represented as `P(x=\"free\"|y=\"HAM\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_probabilities = pd.DataFrame(index=list(corpus), \n",
    "                                         columns=[\"likelihood_given_ham\", \"likelihood_given_spam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I ate dinner early', 'HAM')\n",
      "Spam documents: 0\n",
      "Ham documents: 1 \n",
      "\n",
      "\n",
      "('free money today', 'SPAM')\n",
      "Spam documents: 1\n",
      "Ham documents: 1 \n",
      "\n",
      "\n",
      "('I had a blast', 'HAM')\n",
      "Spam documents: 1\n",
      "Ham documents: 2 \n",
      "\n",
      "\n",
      "('sign up free early today', 'HAM')\n",
      "Spam documents: 1\n",
      "Ham documents: 3 \n",
      "\n",
      "\n",
      "('only free today', 'SPAM')\n",
      "Spam documents: 2\n",
      "Ham documents: 3 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam_documents = 0\n",
    "ham_documents = 0\n",
    "for document in documents:\n",
    "    if document[1] == \"SPAM\":\n",
    "        spam_documents += 1\n",
    "    else:\n",
    "        ham_documents += 1\n",
    "\n",
    "    print(f\"{document}\")\n",
    "    print(f\"Spam documents: {spam_documents}\")\n",
    "    print(f\"Ham documents: {ham_documents} \\n\\n\")\n",
    "    \n",
    "p_ham = ham_documents / (spam_documents + ham_documents)\n",
    "p_spam = spam_documents / (spam_documents + ham_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For word ate, 1 ham out of 3 ham documents.\n",
      "For word ate, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word up, 1 ham out of 3 ham documents.\n",
      "For word up, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word a, 1 ham out of 3 ham documents.\n",
      "For word a, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word only, 0 ham out of 3 ham documents.\n",
      "For word only, 1 spam out of 2 spam documents.\n",
      "\n",
      "For word free, 1 ham out of 3 ham documents.\n",
      "For word free, 2 spam out of 2 spam documents.\n",
      "\n",
      "For word dinner, 1 ham out of 3 ham documents.\n",
      "For word dinner, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word had, 1 ham out of 3 ham documents.\n",
      "For word had, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word blast, 1 ham out of 3 ham documents.\n",
      "For word blast, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word early, 2 ham out of 3 ham documents.\n",
      "For word early, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word sign, 1 ham out of 3 ham documents.\n",
      "For word sign, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word money, 0 ham out of 3 ham documents.\n",
      "For word money, 1 spam out of 2 spam documents.\n",
      "\n",
      "For word today, 1 ham out of 3 ham documents.\n",
      "For word today, 2 spam out of 2 spam documents.\n",
      "\n",
      "For word I, 2 ham out of 3 ham documents.\n",
      "For word I, 0 spam out of 2 spam documents.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in corpus:\n",
    "    \n",
    "    ham_documents_with_word = 0\n",
    "    spam_documents_with_word = 0\n",
    "    \n",
    "    for document in documents:\n",
    "        document_class = document[1]\n",
    "        if word in document[0].split():\n",
    "            if document[1] == \"HAM\":\n",
    "                ham_documents_with_word += 1\n",
    "            else:\n",
    "                spam_documents_with_word += 1\n",
    "    \n",
    "    print(f\"For word {word}, {ham_documents_with_word} ham out of {ham_documents} ham documents.\")\n",
    "    print(f\"For word {word}, {spam_documents_with_word} spam out of {spam_documents} spam documents.\\n\")\n",
    "    conditional_probabilities.loc[word, \"likelihood_given_ham\"] = ham_documents_with_word * 1.0 / ham_documents\n",
    "    conditional_probabilities.loc[word, \"likelihood_given_spam\"] = spam_documents_with_word * 1.0 / spam_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document = \"free today\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood(test_document, conditional_probabilities):\n",
    "    likelihood_ham = 1\n",
    "    likelihood_spam = 1\n",
    "    for word in test_document.split():\n",
    "        likelihood_ham = likelihood_ham * conditional_probabilities.loc[word, \"likelihood_given_ham\"]\n",
    "        likelihood_spam = likelihood_spam * conditional_probabilities.loc[word, \"likelihood_given_spam\"]\n",
    "    \n",
    "    return likelihood_ham, likelihood_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_ham, likelihood_spam = get_likelihood(test_document, conditional_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(likelihood_ham, likelihood_spam, p_ham, p_spam):\n",
    "    posterior_ham = likelihood_ham * p_ham / (likelihood_ham * p_ham + likelihood_spam * p_spam)\n",
    "    posterior_spam = likelihood_spam * p_spam / (likelihood_ham * p_ham + likelihood_spam * p_spam)\n",
    "    return posterior_ham, posterior_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14285714285714285, 0.8571428571428572)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_posterior(likelihood_ham, likelihood_spam, p_ham, p_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_naive_bayes(documents):\n",
    "    corpus = set()\n",
    "    # Build corpus\n",
    "    for document in documents:\n",
    "        text = document[0]\n",
    "        class_value = document[1]\n",
    "        for word in text.split():\n",
    "            corpus.add(word)\n",
    "    \n",
    "    conditional_probabilities = pd.DataFrame(index=list(corpus), \n",
    "                                             columns=[\"likelihood_given_ham\", \"likelihood_given_spam\"])\n",
    "    \n",
    "    spam_documents = 0\n",
    "    ham_documents = 0\n",
    "    for document in documents:\n",
    "        if document[1] == \"SPAM\":\n",
    "            spam_documents += 1\n",
    "        else:\n",
    "            ham_documents += 1\n",
    "    p_ham = ham_documents / (spam_documents + ham_documents)\n",
    "    p_spam = spam_documents / (spam_documents + ham_documents)\n",
    "    \n",
    "    for word in corpus:\n",
    "        ham_documents_with_word = 0\n",
    "        spam_documents_with_word = 0\n",
    "    \n",
    "        for document in documents:\n",
    "            document_class = document[1]\n",
    "            if word in document[0].split():\n",
    "                if document[1] == \"HAM\":\n",
    "                    ham_documents_with_word += 1\n",
    "                else:\n",
    "                    spam_documents_with_word += 1\n",
    "\n",
    "        #print(f\"For word {word}, {ham_documents_with_word} ham out of {ham_documents}.\")\n",
    "        #print(f\"For word {word}, {spam_documents_with_word} spam out of {spam_documents}.\")\n",
    "        conditional_probabilities.loc[word, \"likelihood_given_ham\"] = ham_documents_with_word * 1.0 / ham_documents\n",
    "        conditional_probabilities.loc[word, \"likelihood_given_spam\"] = spam_documents_with_word * 1.0 / spam_documents\n",
    "\n",
    "    \n",
    "    return conditional_probabilities, p_ham, p_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       likelihood_given_ham likelihood_given_spam\n",
       " ate                0.333333                     0\n",
       " up                 0.333333                     0\n",
       " a                  0.333333                     0\n",
       " only                      0                   0.5\n",
       " free               0.333333                     1\n",
       " dinner             0.333333                     0\n",
       " had                0.333333                     0\n",
       " blast              0.333333                     0\n",
       " early              0.666667                     0\n",
       " sign               0.333333                     0\n",
       " money                     0                   0.5\n",
       " today              0.333333                     1\n",
       " I                  0.666667                     0, 0.6, 0.4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_naive_bayes(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Non-Existent Words\n",
    "\n",
    "From [Sebastian Raschka, Python Machine Learning](https://arxiv.org/pdf/1410.5329.pdf)\n",
    "![Correlations](images/smoothing.png \"Visualization of various r values for Pearson correlation coefficient\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
