{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Word-Count\" data-toc-modified-id=\"Word-Count-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Word Count</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-Counter\" data-toc-modified-id=\"Using-Counter-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Using <code>Counter</code></a></span></li><li><span><a href=\"#Adding-Word-Counts-From-Two-Distinct-Datasets-Together\" data-toc-modified-id=\"Adding-Word-Counts-From-Two-Distinct-Datasets-Together-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Adding Word Counts From Two Distinct Datasets Together</a></span></li></ul></li><li><span><a href=\"#Removing-Stopwords-Using-gensim\" data-toc-modified-id=\"Removing-Stopwords-Using-gensim-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Removing Stopwords Using <code>gensim</code></a></span></li><li><span><a href=\"#Finding-Similar-Word-Matches-Using-difflib\" data-toc-modified-id=\"Finding-Similar-Word-Matches-Using-difflib-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Finding Similar Word Matches Using <code>difflib</code></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count\n",
    "## Using `Counter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normal dictionary object will return a key error if you do not first initialize the key value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'yu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5b4cffd8d4f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mordinary_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mordinary_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"yu\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'yu'"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "ordinary_dict: Dict = dict()\n",
    "ordinary_dict[\"yu\"] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Counter` object in `collections` has a default value of 0 for every key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'yu': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "counter[\"yu\"] += 1\n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, the you can pass in a list of strings to the `Counter` constructor, as well as calling the\n",
    "`most_common` method to get the most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 7363), ('and', 4727), ('of', 3944), ('to', 3398), ('a', 2792)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "words: List[str] = open(\"tale-of-two-cities.txt\").read().split()\n",
    "dickens_counter = Counter(words)\n",
    "dickens_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also quickly use this counter to find the percentage of words in a corpus that belong to a certain word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054036400998091885"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dickens_counter[\"the\"] / sum(dickens_counter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Word Counts From Two Distinct Datasets Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add two `Counter` objects together to get their combined counts. In this example, we'll load in the `fraudulent_emails.txt` dataset and start a new counter called `email_counter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 141), ('to', 116), ('I', 115), ('of', 80), ('in', 80)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_counter = Counter(open(\"fraudulent_emails.txt\").read().split())\n",
    "email_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 7504), ('and', 4789), ('of', 4024), ('to', 3514), ('a', 2834)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_counter: Counter = dickens_counter + email_counter\n",
    "combined_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also subtract counts from one dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 141), ('to', 116), ('I', 115), ('of', 80), ('in', 80)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get back the original email_counter\n",
    "(combined_counter - dickens_counter).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stopwords Using `gensim`\n",
    "\n",
    "Removing stopwords in `nltk` often means you first have to tokenize the document into distinct tokens, then run each token through to check if it is a stopword. Another commonly used NLP library in Python, `gensim`, has a helper function to do this all in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rendered manner desperate, state beckoning conductor, drew neck arm shook shoulder, lifted little, hurried room. He sat door, held her, clinging him.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "text = '''\n",
    "Rendered in a manner desperate, by her state and by the beckoning of their conductor,\n",
    "he drew over his neck the arm that shook upon his shoulder, lifted her a little, and hurried \n",
    "her into the room. He sat her down just within the door, and held her, clinging to him.\n",
    "'''\n",
    "processed_text = remove_stopwords(text)\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, however, this only works well if you are happy with Gensim's only predefined list of stopwords. To inspect what stopwords are used in Gensim, use\n",
    "```python\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "print(STOPWORDS)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Similar Word Matches Using `difflib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within Python's Standard Library, the `difflib` has a variety of tools for helping identify differences between text and content. It uses an algorithm called the **Ratcliff-Obershelp algorithm**, which is described in brief below:\n",
    "\n",
    "> The idea is to find the longest contiguous matching subsequence that contains no “junk” elements; these “junk” elements are ones that are uninteresting in some sense, such as blank lines or whitespace. (Handling junk is an extension to the Ratcliff and Obershelp algorithm.) The same idea is then applied recursively to the pieces of the sequences to the left and to the right of the matching subsequence. This does not yield minimal edit sequences, but does tend to yield matches that “look right” to people. [Link](https://docs.python.org/3/library/difflib.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads in the top 20k most popular words in the English language\n",
    "words = set(map(lambda word: word.replace(\"\\n\", \"\"), open(\"20k.txt\").readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight', 'naughty', 'knights']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "w = \"knaght\"\n",
    "difflib.get_close_matches(w, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine this with a tokenizer to create your own (very basic) spellcheck function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He is a crazy person'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def spellcheck_document(text):\n",
    "    new_tokens = []\n",
    "    for token in word_tokenize(text):\n",
    "        matches = difflib.get_close_matches(token.lower(), words, n=1, cutoff=0.7)\n",
    "        if len(matches) == 0 or token.lower() in words:\n",
    "            new_tokens.append(token)\n",
    "        else:\n",
    "            new_tokens.append(matches[0])\n",
    "    return \" \".join(new_tokens)\n",
    "spellcheck_document(\"He is a craezy perzon\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "126.534px",
    "left": "967.989px",
    "top": "85.9517px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
