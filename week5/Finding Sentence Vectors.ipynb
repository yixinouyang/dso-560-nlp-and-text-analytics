{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've only studied word embeddings, where each word is represented by a vector of numbers. For instance, the word cat might be represented as \n",
    "\n",
    "```python\n",
    "cat = [0.23, 0.10, -0.23, -0.01, 0.91, 1.2, 1.01, -0.92]\n",
    "```\n",
    "\n",
    "But how would you represent a **sentence**? There are many different ways to represent sentences, but the simplest, and often very effective way is to **take the average of all the word embeddings of that sentence**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note:\n",
    "\n",
    " Before you start this next portion, download `en_core_web_md` for spacy - the `en_core_web_sm` model we used in class is good as a starter introduction to word embeddings, but won't give you as great results in the long run:\n",
    " \n",
    ">*To make them compact and fast, spaCy’s small models (all packages that end in `sm`) don’t ship with **true word vectors**, and only include context-sensitive tensors. This means you can still use the `similarity()` methods to compare documents, spans and tokens – but the result won’t be as good, and individual tokens won’t have any vectors assigned. So in order to use real word vectors, you need to download a larger model*.\n",
    "\n",
    "You can download the larger model in Python by using `python -m spacy en_core_web_md`. In your Jupyter notebook cell, you can also type the command `!{sys.executable} -m spacy download en_core_web_md` in a cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in spacy\n",
    "import en_core_web_md\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence A:\n",
      "I's word embedding: [ 0.18733   0.40595  -0.51174  -0.55482   0.039716  0.12887 ]'\n",
      "watched's word embedding: [ 0.08763  -0.41748  -0.33357  -0.080973 -0.089307  0.12784 ]'\n",
      "a's word embedding: [ 0.043798  0.024779 -0.20937   0.49745   0.36019  -0.37503 ]'\n",
      "movie's word embedding: [ 0.2071  -0.47656  0.15479 -0.38965  0.48447  0.59815]'\n",
      "with's word embedding: [-0.099534  0.028202 -0.23189   0.094477  0.12191  -0.18962 ]'\n",
      "my's word embedding: [ 0.08649  0.14503 -0.4902   0.34224  0.36343  0.10046]'\n",
      "friend's word embedding: [ 0.07781   0.17561  -0.59164   0.25467   0.35536  -0.012292]'\n",
      ".'s word embedding: [ 0.012001  0.20751  -0.12578  -0.59325   0.12525   0.15975 ]'\n",
      "\n",
      "Sentence B:\n",
      "I's word embedding: [ 0.18733   0.40595  -0.51174  -0.55482   0.039716  0.12887 ]'\n",
      "saw's word embedding: [ 0.028726  0.15006  -0.19278  -0.13624   0.21288   0.085543]'\n",
      "a's word embedding: [ 0.043798  0.024779 -0.20937   0.49745   0.36019  -0.37503 ]'\n",
      "film's word embedding: [ 0.30708 -0.30603  0.43486  0.13947  0.4484   0.53089]'\n",
      "with's word embedding: [-0.099534  0.028202 -0.23189   0.094477  0.12191  -0.18962 ]'\n",
      "my's word embedding: [ 0.08649  0.14503 -0.4902   0.34224  0.36343  0.10046]'\n",
      "buddy's word embedding: [-0.052945 -0.32081  -0.5023    0.001422  0.32335   0.15518 ]'\n"
     ]
    }
   ],
   "source": [
    "sentenceA = \"I watched a movie with my friend.\"\n",
    "\n",
    "sentenceA_tokens = nlp(sentenceA)\n",
    "print(\"\\nSentence A:\")\n",
    "for token in nlp(sentenceA): # I am only going to show the first 6 values of the word embedding, but \n",
    "    # remember that the embedding itself is usually 50, 100, 300, 500 elements long (in Spacy's case, 384)\n",
    "    print(f\"{token}'s word embedding: {token.vector[:6]}'\")\n",
    "print(\"\\nSentence B:\")\n",
    "for token in nlp(sentenceB):\n",
    "      print(f\"{token}'s word embedding: {token.vector[:6]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you had used `en_core_web_sm`, spacy will generate your word embeddings on the fly, the same word, like `I` might have slightly different embedding values! In `en_core_web_md`, spacy downloads and uses pre-trained embeddings that are fixed and more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the sentence vector for sentence A, sum each of the words in sentence A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to find the sentence embedding of sentence A\n",
    "# create a 300 length word embedding (spacy's en_core_web_md model uses 300-dimensional word embeddings)\n",
    "running_total = np.zeros(300) \n",
    "for token in nlp(sentenceA):\n",
    "    running_total += token.vector # add the word embeddings to the running total\n",
    "\n",
    "# divide by the total number of words in sentence to get the \"average embedding\"\n",
    "sentence_embedding = running_total / len(nlp(sentenceA)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07532812,  0.01163013, -0.292425  , -0.053732  ,  0.22012738,\n",
       "        0.067266  ,  0.06414513, -0.40898649,  0.07971475,  2.07558748])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the first 10 values of the 300-dimensional word embeddings in en_core_web_md for sentence A\n",
    "\n",
    "sentence_embedding[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's actually an even easier way to do this in spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07532812,  0.01163013, -0.29242504, -0.05373199,  0.22012737,\n",
       "        0.067266  ,  0.06414513, -0.40898648,  0.07971475,  2.0755873 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nlp(sentenceA)\n",
    "tokens.vector[:10] # the same as the above, when we got the sentence embeddings ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceA_embedding = nlp(sentenceA).vector\n",
    "sentenceB_embedding = nlp(sentenceB).vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between sentence A and sentence B is 0.9586231708526611\n"
     ]
    }
   ],
   "source": [
    "similarity = 1 - cosine(sentenceA_embedding, sentenceB_embedding)\n",
    "print(f\"The similarity between sentence A and sentence B is {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between sentence C and sentence A is 0.8648006319999695\n",
      "The similarity between sentence C and sentence B is 0.8382346630096436\n"
     ]
    }
   ],
   "source": [
    "sentenceC = \"I drank a watermelon with my dog.\" # structurally, this is extremely similar to sentence A and B. \n",
    "# however, semantically, it is extremely different! Let's prove that word embeddings can be used to tell that\n",
    "# sentenceC is not as similar to A and B.\n",
    "\n",
    "sentenceC_embedding = nlp(sentenceC).vector\n",
    "similarity = 1 - cosine(sentenceC_embedding, sentenceA_embedding)\n",
    "print(f\"The similarity between sentence C and sentence A is {similarity}\")\n",
    "similarity = 1 - cosine(sentenceC_embedding, sentenceB_embedding)\n",
    "print(f\"The similarity between sentence C and sentence B is {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we substitute in `pal` for `dog`? Our word count models would not have picked up on any real difference, since `pal` just another word to be counted. However, semantically, `pal` is an informal name for a friend, and substituting in this new word will increase our similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between sentence C and sentence A is 0.8804790377616882\n",
      "The similarity between sentence C and sentence B is 0.8583351373672485\n"
     ]
    }
   ],
   "source": [
    "sentenceC = \"I drank a watermelon with my pal.\"\n",
    "\n",
    "sentenceC_embedding = nlp(sentenceC).vector\n",
    "similarity = 1 - cosine(sentenceC_embedding, sentenceA_embedding)\n",
    "print(f\"The similarity between sentence C and sentence A is {similarity}\")\n",
    "similarity = 1 - cosine(sentenceC_embedding, sentenceB_embedding)\n",
    "print(f\"The similarity between sentence C and sentence B is {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between sentence C and sentence A is 0.9236767292022705\n",
      "The similarity between sentence C and sentence B is 0.9158568978309631\n"
     ]
    }
   ],
   "source": [
    "sentenceC = \"I saw a watermelon with my pal.\"\n",
    "\n",
    "sentenceC_embedding = nlp(sentenceC).vector\n",
    "similarity = 1 - cosine(sentenceC_embedding, sentenceA_embedding)\n",
    "print(f\"The similarity between sentence C and sentence A is {similarity}\")\n",
    "similarity = 1 - cosine(sentenceC_embedding, sentenceB_embedding)\n",
    "print(f\"The similarity between sentence C and sentence B is {similarity}\")\n",
    "# Notice the even higher similarity after I substitute in \"saw\", a synonym for watched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
