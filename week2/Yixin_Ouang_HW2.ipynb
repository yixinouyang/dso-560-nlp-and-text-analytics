{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yixin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yixin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yixin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from fuzzywuzzy import fuzz\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Using the **McDonalds Yelp Review CSV file**, **process the reviews**.\n",
    "This means you should think briefly about:\n",
    "* what stopwords to remove (should you add any custom stopwords to the set? Remove any stopwords?)\n",
    "* what regex cleaning you may need to perform (for example, are there different ways of saying `hamburger` that you need to account for?)\n",
    "* stemming/lemmatization (explain in your notebook why you used stemming versus lemmatization). \n",
    "\n",
    "Next, **count-vectorize the dataset**. Use the **`sklearn.feature_extraction.text.CountVectorizer`** examples from `Linear Algebra, Distance and Similarity (Completed).ipynb` and `Text Preprocessing Techniques (Completed).ipynb` (read the last section, `Vectorization Techniques`).\n",
    "\n",
    "I do not want redundant features - for instance, I do not want `hamburgers` and `hamburger` to be two distinct columns in your document-term matrix. Therefore, I'll be taking a look to make sure you've properly performed your cleaning, stopword removal, etc. to reduce the number of dimensions in your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_review_df = pd.read_csv('mcdonalds-yelp-negative-reviews.csv', encoding = 'latin-1')\n",
    "negative_review_df['lower_case'] = negative_review_df['review'].str.lower()\n",
    "negative_review_df['timestamp'] = negative_review_df['lower_case'].str.replace(\n",
    "    r'(?:[0-1][0-9]:[0-5][0-9])|(?:[0-1]?[0-9]?:?[0-5]?[0-9](?:ish)?\\s?(?:am|pm))','TIMESTAMP_TOKEN')\n",
    "negative_review_df['stopword'] = negative_review_df['timestamp'].str.replace(\n",
    "    r'\\b('+'|'.join(stopword_list)+r')\\b','')\n",
    "negative_review_df['word_list'] = negative_review_df['stopword'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_list = set(negative_review_df['stopword'].str.findall(r'['+string.punctuation+r']+').explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>city</th>\n",
       "      <th>review</th>\n",
       "      <th>lower_case</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>stopword</th>\n",
       "      <th>word_list</th>\n",
       "      <th>stem</th>\n",
       "      <th>join</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>679455653</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>I'm not a huge mcds lover, but I've been to be...</td>\n",
       "      <td>i'm not a huge mcds lover, but i've been to be...</td>\n",
       "      <td>i'm not a huge mcds lover, but i've been to be...</td>\n",
       "      <td>'   huge mcds lover,  '   better ones.    far ...</td>\n",
       "      <td>[', huge, mcds, lover, ,, ', better, ones, ., ...</td>\n",
       "      <td>[huge, mcd, lover, better, one, far, worst, on...</td>\n",
       "      <td>huge mcd lover better one far worst one ever f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>679455654</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Terrible customer service. I came in at 9:30pm...</td>\n",
       "      <td>terrible customer service. i came in at 9:30pm...</td>\n",
       "      <td>terrible customer service. i came in at TIMEST...</td>\n",
       "      <td>terrible customer service.  came   TIMESTAMP_T...</td>\n",
       "      <td>[terrible, customer, service, ., came, TIMESTA...</td>\n",
       "      <td>[terribl, custom, servic, came, timestamp_toke...</td>\n",
       "      <td>terribl custom servic came timestamp_token sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>679455655</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>First they \"lost\" my order, actually they gave...</td>\n",
       "      <td>first they \"lost\" my order, actually they gave...</td>\n",
       "      <td>first they \"lost\" my order, actually they gave...</td>\n",
       "      <td>first  \"lost\"  order, actually  gave   someone...</td>\n",
       "      <td>[first, ``, lost, '', order, ,, actually, gave...</td>\n",
       "      <td>[first, ``, lost, order, actual, gave, someon,...</td>\n",
       "      <td>first `` lost order actual gave someon one els...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>679455656</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>I see I'm not the only one giving 1 star. Only...</td>\n",
       "      <td>i see i'm not the only one giving 1 star. only...</td>\n",
       "      <td>i see i'm not the only one giving 1 star. only...</td>\n",
       "      <td>see '    one giving 1 star.       -25 star!!!...</td>\n",
       "      <td>[see, ', one, giving, 1, star, ., -25, star, !...</td>\n",
       "      <td>[see, one, give, 1, star, -25, star, need, say]</td>\n",
       "      <td>see one give 1 star -25 star need say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>679455657</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Well, it's McDonald's, so you know what the fo...</td>\n",
       "      <td>well, it's mcdonald's, so you know what the fo...</td>\n",
       "      <td>well, it's mcdonald's, so you know what the fo...</td>\n",
       "      <td>well, ' mcdonald',   know   food .  review ref...</td>\n",
       "      <td>[well, ,, ', mcdonald, ', ,, know, food, ., re...</td>\n",
       "      <td>[well, mcdonald, know, food, review, reflect, ...</td>\n",
       "      <td>well mcdonald know food review reflect sole po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>679500008</td>\n",
       "      <td>Portland</td>\n",
       "      <td>I enjoyed the part where I repeatedly asked if...</td>\n",
       "      <td>i enjoyed the part where i repeatedly asked if...</td>\n",
       "      <td>i enjoyed the part where i repeatedly asked if...</td>\n",
       "      <td>enjoyed  part   repeatedly asked     right sa...</td>\n",
       "      <td>[enjoyed, part, repeatedly, asked, right, sauc...</td>\n",
       "      <td>[enjoy, part, repeatedli, ask, right, sauc, 4,...</td>\n",
       "      <td>enjoy part repeatedli ask right sauc 4 time fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>679500224</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Worst McDonalds I've been in in a long time! D...</td>\n",
       "      <td>worst mcdonalds i've been in in a long time! d...</td>\n",
       "      <td>worst mcdonalds i've been in in a long time! d...</td>\n",
       "      <td>worst mcdonalds '     long time! dirt everywhe...</td>\n",
       "      <td>[worst, mcdonalds, ', long, time, !, dirt, eve...</td>\n",
       "      <td>[worst, mcdonald, long, time, dirt, everywher,...</td>\n",
       "      <td>worst mcdonald long time dirt everywher food b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>679500608</td>\n",
       "      <td>New York</td>\n",
       "      <td>When I am really craving for McDonald's, this ...</td>\n",
       "      <td>when i am really craving for mcdonald's, this ...</td>\n",
       "      <td>when i am really craving for mcdonald's, this ...</td>\n",
       "      <td>really craving  mcdonald',  seems    closes...</td>\n",
       "      <td>[really, craving, mcdonald, ', ,, seems, close...</td>\n",
       "      <td>[realli, crave, mcdonald, seem, closest, big, ...</td>\n",
       "      <td>realli crave mcdonald seem closest big fan fas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>679501257</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Two points right out of the gate: 1. Thuggery ...</td>\n",
       "      <td>two points right out of the gate: 1. thuggery ...</td>\n",
       "      <td>two points right out of the gate: 1. thuggery ...</td>\n",
       "      <td>two points right    gate: 1. thuggery knows  r...</td>\n",
       "      <td>[two, points, right, gate, :, 1., thuggery, kn...</td>\n",
       "      <td>[two, point, right, gate, 1., thuggeri, know, ...</td>\n",
       "      <td>two point right gate 1. thuggeri know race lil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>679501402</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>I wanted to grab breakfast one morning before ...</td>\n",
       "      <td>i wanted to grab breakfast one morning before ...</td>\n",
       "      <td>i wanted to grab breakfast one morning before ...</td>\n",
       "      <td>wanted  grab breakfast one morning  work sinc...</td>\n",
       "      <td>[wanted, grab, breakfast, one, morning, work, ...</td>\n",
       "      <td>[want, grab, breakfast, one, morn, work, sinc,...</td>\n",
       "      <td>want grab breakfast one morn work sinc across ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1525 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       _unit_id         city  \\\n",
       "0     679455653      Atlanta   \n",
       "1     679455654      Atlanta   \n",
       "2     679455655      Atlanta   \n",
       "3     679455656      Atlanta   \n",
       "4     679455657      Atlanta   \n",
       "...         ...          ...   \n",
       "1520  679500008     Portland   \n",
       "1521  679500224      Houston   \n",
       "1522  679500608     New York   \n",
       "1523  679501257      Chicago   \n",
       "1524  679501402  Los Angeles   \n",
       "\n",
       "                                                 review  \\\n",
       "0     I'm not a huge mcds lover, but I've been to be...   \n",
       "1     Terrible customer service. I came in at 9:30pm...   \n",
       "2     First they \"lost\" my order, actually they gave...   \n",
       "3     I see I'm not the only one giving 1 star. Only...   \n",
       "4     Well, it's McDonald's, so you know what the fo...   \n",
       "...                                                 ...   \n",
       "1520  I enjoyed the part where I repeatedly asked if...   \n",
       "1521  Worst McDonalds I've been in in a long time! D...   \n",
       "1522  When I am really craving for McDonald's, this ...   \n",
       "1523  Two points right out of the gate: 1. Thuggery ...   \n",
       "1524  I wanted to grab breakfast one morning before ...   \n",
       "\n",
       "                                             lower_case  \\\n",
       "0     i'm not a huge mcds lover, but i've been to be...   \n",
       "1     terrible customer service. i came in at 9:30pm...   \n",
       "2     first they \"lost\" my order, actually they gave...   \n",
       "3     i see i'm not the only one giving 1 star. only...   \n",
       "4     well, it's mcdonald's, so you know what the fo...   \n",
       "...                                                 ...   \n",
       "1520  i enjoyed the part where i repeatedly asked if...   \n",
       "1521  worst mcdonalds i've been in in a long time! d...   \n",
       "1522  when i am really craving for mcdonald's, this ...   \n",
       "1523  two points right out of the gate: 1. thuggery ...   \n",
       "1524  i wanted to grab breakfast one morning before ...   \n",
       "\n",
       "                                              timestamp  \\\n",
       "0     i'm not a huge mcds lover, but i've been to be...   \n",
       "1     terrible customer service. i came in at TIMEST...   \n",
       "2     first they \"lost\" my order, actually they gave...   \n",
       "3     i see i'm not the only one giving 1 star. only...   \n",
       "4     well, it's mcdonald's, so you know what the fo...   \n",
       "...                                                 ...   \n",
       "1520  i enjoyed the part where i repeatedly asked if...   \n",
       "1521  worst mcdonalds i've been in in a long time! d...   \n",
       "1522  when i am really craving for mcdonald's, this ...   \n",
       "1523  two points right out of the gate: 1. thuggery ...   \n",
       "1524  i wanted to grab breakfast one morning before ...   \n",
       "\n",
       "                                               stopword  \\\n",
       "0     '   huge mcds lover,  '   better ones.    far ...   \n",
       "1     terrible customer service.  came   TIMESTAMP_T...   \n",
       "2     first  \"lost\"  order, actually  gave   someone...   \n",
       "3      see '    one giving 1 star.       -25 star!!!...   \n",
       "4     well, ' mcdonald',   know   food .  review ref...   \n",
       "...                                                 ...   \n",
       "1520   enjoyed  part   repeatedly asked     right sa...   \n",
       "1521  worst mcdonalds '     long time! dirt everywhe...   \n",
       "1522     really craving  mcdonald',  seems    closes...   \n",
       "1523  two points right    gate: 1. thuggery knows  r...   \n",
       "1524   wanted  grab breakfast one morning  work sinc...   \n",
       "\n",
       "                                              word_list  \\\n",
       "0     [', huge, mcds, lover, ,, ', better, ones, ., ...   \n",
       "1     [terrible, customer, service, ., came, TIMESTA...   \n",
       "2     [first, ``, lost, '', order, ,, actually, gave...   \n",
       "3     [see, ', one, giving, 1, star, ., -25, star, !...   \n",
       "4     [well, ,, ', mcdonald, ', ,, know, food, ., re...   \n",
       "...                                                 ...   \n",
       "1520  [enjoyed, part, repeatedly, asked, right, sauc...   \n",
       "1521  [worst, mcdonalds, ', long, time, !, dirt, eve...   \n",
       "1522  [really, craving, mcdonald, ', ,, seems, close...   \n",
       "1523  [two, points, right, gate, :, 1., thuggery, kn...   \n",
       "1524  [wanted, grab, breakfast, one, morning, work, ...   \n",
       "\n",
       "                                                   stem  \\\n",
       "0     [huge, mcd, lover, better, one, far, worst, on...   \n",
       "1     [terribl, custom, servic, came, timestamp_toke...   \n",
       "2     [first, ``, lost, order, actual, gave, someon,...   \n",
       "3       [see, one, give, 1, star, -25, star, need, say]   \n",
       "4     [well, mcdonald, know, food, review, reflect, ...   \n",
       "...                                                 ...   \n",
       "1520  [enjoy, part, repeatedli, ask, right, sauc, 4,...   \n",
       "1521  [worst, mcdonald, long, time, dirt, everywher,...   \n",
       "1522  [realli, crave, mcdonald, seem, closest, big, ...   \n",
       "1523  [two, point, right, gate, 1., thuggeri, know, ...   \n",
       "1524  [want, grab, breakfast, one, morn, work, sinc,...   \n",
       "\n",
       "                                                   join  \n",
       "0     huge mcd lover better one far worst one ever f...  \n",
       "1     terribl custom servic came timestamp_token sto...  \n",
       "2     first `` lost order actual gave someon one els...  \n",
       "3                 see one give 1 star -25 star need say  \n",
       "4     well mcdonald know food review reflect sole po...  \n",
       "...                                                 ...  \n",
       "1520  enjoy part repeatedli ask right sauc 4 time fu...  \n",
       "1521  worst mcdonald long time dirt everywher food b...  \n",
       "1522  realli crave mcdonald seem closest big fan fas...  \n",
       "1523  two point right gate 1. thuggeri know race lil...  \n",
       "1524  want grab breakfast one morn work sinc across ...  \n",
       "\n",
       "[1525 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "negative_review_df['stem'] = negative_review_df['word_list'].apply(lambda x: [stemmer.stem(word) for word in x if word not in punctuation_list])\n",
    "negative_review_df['join'] = negative_review_df['stem'].apply(lambda x: ' '.join(x))\n",
    "negative_review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>05</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummi</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombi</th>\n",
       "      <th>î_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1525 rows × 3130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  05  08  09  10  100  10th  11  12  13  ...  york  young  younger  \\\n",
       "0      0   0   0   0   0    0     0   0   0   0  ...     0      0        0   \n",
       "1      0   0   0   0   0    0     0   0   0   0  ...     0      0        0   \n",
       "2      0   0   0   0   0    0     0   0   0   0  ...     0      0        0   \n",
       "3      0   0   0   0   0    0     0   0   0   0  ...     0      0        0   \n",
       "4      0   0   0   0   0    1     0   0   1   0  ...     0      0        0   \n",
       "...   ..  ..  ..  ..  ..  ...   ...  ..  ..  ..  ...   ...    ...      ...   \n",
       "1520   0   0   0   0   0    0     0   0   0   0  ...     0      0        0   \n",
       "1521   0   0   0   0   0    0     0   0   0   0  ...     0      0        0   \n",
       "1522   0   0   0   0   0    0     0   0   0   0  ...     0      0        0   \n",
       "1523   0   0   0   0   0    0     0   0   0   0  ...     0      0        0   \n",
       "1524   0   0   0   0   0    0     0   0   0   0  ...     0      0        0   \n",
       "\n",
       "      yuck  yum  yummi  yup  zero  zombi  î_  \n",
       "0        0    0      0    0     0      0   0  \n",
       "1        0    0      0    0     0      0   0  \n",
       "2        0    0      0    0     0      0   0  \n",
       "3        0    0      0    0     0      0   0  \n",
       "4        0    0      0    0     0      0   0  \n",
       "...    ...  ...    ...  ...   ...    ...  ..  \n",
       "1520     0    0      0    0     0      0   0  \n",
       "1521     0    0      0    0     0      0   0  \n",
       "1522     0    0      0    0     0      0   0  \n",
       "1523     0    0      0    0     1      0   0  \n",
       "1524     0    0      0    0     0      0   0  \n",
       "\n",
       "[1525 rows x 3130 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=2)\n",
    "X = vectorizer.fit_transform(negative_review_df['join'])\n",
    "X = X.toarray()\n",
    "corpus_df = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "1. Read the data and lowercase \n",
    "2. Replace timestamp value by Regular Expression\n",
    "3. Stopword removal\n",
    "4. Stemming because I want to lower the number of features more.\n",
    "5. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. **Stopwords, Stemming, Lemmatization Practice**\n",
    "\n",
    "Using the `tale-of-two-cities.txt` file from Week 1:\n",
    "* Count-vectorize the corpus. Treat each sentence as a document.\n",
    "\n",
    "How many features (dimensions) do you get when you:\n",
    "* Perform **stemming and then count-vectorization\n",
    "* Perform **lemmatization** and then **count-vectorization**.\n",
    "* Perform **lemmatization**, remove **stopwords**, and then perform **count-vectorization**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('tale-of-two-cities.txt', \"r\", encoding='utf8').read().replace('\\n',' ')\n",
    "sent_text = nltk.sent_tokenize(text) # this gives us a list of sentences\n",
    "word_list_sent = [word_tokenize(sent) for sent in sent_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stem_only = []\n",
    "for sent in word_list_sent:\n",
    "    stem_only.append([stemmer.stem(word) for word in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_only = []\n",
    "for sent in word_list_sent:\n",
    "    lemma_only.append([lemmatizer.lemmatize(word) for word in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english')\n",
    "lemma_stop = []\n",
    "for sent in word_list_sent:\n",
    "    lemma_stop.append([lemmatizer.lemmatize(word) for word in sent if word not in stopword_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features(word_list_sent):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform([' '.join(sent) for sent in word_list_sent])\n",
    "    return len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6659"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_features(stem_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8910"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_features(lemma_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8897"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_features(lemma_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: As we can see here, stemming have the lowest number of features, while lemmatization have much more. Also removing stopword will decrease the number a little bit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
